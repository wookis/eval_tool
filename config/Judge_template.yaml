Knowledge_template: |
  [Role]
  당신은 대규모 언어 모델이 생성한 답변의 '사실 정확성'을 평가하는 전문가입니다.
  
  [reference_data]가 제공된다면 이를 **최우선 기준**으로 삼아 [응답 프롬프트]의 품질을 평가하세요.  
  reference_data가 없을 경우, 일반적으로 알려진 사실/지식에 기반해 평가하세요.

  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위에서 점수화하세요.  
    1. 사실 정확성 (가중치 0.30) : 제시된 정보가 실제 사실과 얼마나 일치하는가  
    2. 세부 정보 신뢰도 (가중치 0.15) : 날짜, 수치, 고유명사 등의 정확성  
    3. 근거 제시 여부 (가중치 0.15) : 정보에 대한 출처나 논리적 근거가 명확한가  
    4. 오정보 여부 (가중치 0.20) : 잘못되거나 왜곡된 정보가 없는가  
    5. 완전성 (가중치 0.20) : 질문에 필요한 핵심 정보를 빠짐없이 제공했는가  

  [총점 산출 방식]  
  - 각 항목의 점수를 0~10 사이 값으로 매기고, 가중치를 곱합니다.  
  - 모든 항목의 "(점수 × 가중치)" 합을 "(가중치 총합)"으로 나눕니다.  
  - 이 값을 다시 10으로 나누어 0~1 범위로 정규화합니다.  
  - 최종 총점은 **0~1 범위 소수점 2자리**로 표시하세요.  
  - 예: 총점 = (Σ(점수 × 가중치) / Σ(가중치)) / 10  

  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}


  [평가 결과]
  평가 결과는 반드시 아래와 같은 **JSON** 양식으로 제공해줘야 합니다.
  양식은 절대 변경하지 마세요.
  ```json
  {
    "사실 정확성": <0-10 점수>, 
    "세부 정보 신뢰도": <0-10 점수>,
    "근거 제시 여부": <0-10 점수>,
    "오정보 여부": <0-10 점수>,
    "완전성": <0-10 점수>,
    "총점" : <0-1범위 소수점 2자리, 가중 평균>,
    "평가의견" : <brief reason summary>
  }
  ```

Reason_template: |
  [Role]
  당신은 대규모 언어 모델이 생성한 '단계별 설명'의 품질을 평가하는 전문가입니다.

  [reference_data]가 제공된다면 이를 **최우선 기준**으로 삼아 [응답 프롬프트]의 품질을 평가하세요.  
  reference_data가 없을 경우, 일반적으로 알려진 사실/지식에 기반해 평가하세요.

  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위에서 점수화하세요.  
  - 논리적 순서 (가중치 0.25): 절차가 올바른 순서로 진행되는가  
  - 단계 간 연결성 (가중치 0.15): 각 단계가 다음 단계와 자연스럽게 이어지는가  
  - 과정의 정확성 (가중치 0.25): 각 단계의 설명이 사실 및 논리적으로 정확한가  
  - 단계 완전성 (가중치 0.10): 필요한 단계가 빠지지 않았는가  
  - 최종 결과의 정확성 (가중치 0.25): 제시된 최종 답 또는 결과가 옳은가  

  [총점 산출 방식]  
  - 각 항목의 점수를 0~10 사이 값으로 매기고, 가중치를 곱합니다.  
  - 모든 항목의 "(점수 × 가중치)" 합을 "(가중치 총합)"으로 나눕니다.  
  - 이 값을 다시 10으로 나누어 0~1 범위로 정규화합니다.  
  - 최종 총점은 **0~1 범위 소수점 2자리**로 표시하세요.  
  - 예: 총점 = (Σ(점수 × 가중치) / Σ(가중치)) / 10  

  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}
  
  [평가 결과]
  평가 결과는 반드시 아래와 같은 **JSON** 양식으로 제공해줘야 합니다.
  양식은 절대 변경하지 마세요.
  ```json
  {
    "논리적 순서" : <0-10 점수>,
    "단계 간 연결성" : <0-10 점수>,
    "과정의 정확성" : <0-10 점수>,
    "단계 완전성" : <0-10 점수>,
    "최종 결과의 정확성" : <0-10 점수>,    
    "총점" : <0-1범위 소수점 2자리, 가중 평균>,
    "평가의견" : <brief reason summary>
  }
  ```

Creative_template: |
  [Role]
  당신은 대규모 언어 모델이 작성한 '창작/서술형 콘텐츠'의 품질을 평가하는 전문가입니다.

  [reference_data]가 제공된다면 이를 **최우선 기준**으로 삼아 [응답 프롬프트]의 품질을 평가하세요.  
  reference_data가 없을 경우, 일반적으로 알려진 사실/지식에 기반해 평가하세요.

  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위에서 점수화하세요.  
  - 창의성 (가중치 0.35): 아이디어가 독창적이고 새로운가  
  - 구성 및 흐름 (가중치 0.20): 글의 구조와 전개가 자연스럽고 일관성 있는가  
  - 몰입도 (가중치 0.20): 독자의 흥미를 끌고 유지하는가  
  - 감정 전달력 (가중치 0.15): 의도된 분위기나 감정을 효과적으로 전달하는가  
  - 요청 톤/스타일 반영 (가중치 0.10): 프롬프트에서 요구한 스타일이나 어조를 잘 반영했는가 

  [총점 산출 방식]  
  - 각 항목의 점수를 0~10 사이 값으로 매기고, 가중치를 곱합니다.  
  - 모든 항목의 "(점수 × 가중치)" 합을 "(가중치 총합)"으로 나눕니다.  
  - 이 값을 다시 10으로 나누어 0~1 범위로 정규화합니다.  
  - 최종 총점은 **0~1 범위 소수점 2자리**로 표시하세요.  
  - 예: 총점 = (Σ(점수 × 가중치) / Σ(가중치)) / 10  

  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}

  [평가 결과]
  평가 결과는 반드시 아래와 같은 **JSON** 양식으로 제공해줘야 합니다.
  양식은 절대 변경하지 마세요.
  ```json
  {
    "창의성" : <0-10 점수>,
    "문장 구성" : <0-10 점수>,
    "몰입도" : <0-10 점수>,
    "감정 전달력" : <0-10 점수>,
    "요청 톤/스타일 반영" : <0-10 점수>,
    "총점" : <0-1범위 소수점 2자리, 가중 평균>,
    "평가의견" : <brief reason summary>
  }
  ```

Summary_template: |
  [Role]
  당신은 대규모 언어 모델이 작성한 '요약/요약형 콘텐츠'의 품질을 평가하는 전문가입니다.

  [reference_data]가 제공된다면 이를 **최우선 기준**으로 삼아 [응답 프롬프트]의 품질을 평가하세요.  
  reference_data가 없을 경우, 일반적으로 알려진 사실/지식에 기반해 평가하세요.

  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위에서 점수화하세요.  
  1. 핵심 포함 여부 (가중치 0.25) : 원문에서 중요한 정보가 모두 포함되었는가
  2. 불필요 정보 배제 (가중치 0.25) : 사소하거나 관련 없는 내용이 제거되었는가
  3. 간결성 (가중치 0.20) : 불필요한 장황함 없이 간결하게 작성되었는가
  4. 논리적 흐름 (가중치 0.20) : 요약된 내용이 자연스럽게 이어지는가
  5. 요청 수준 적합성 (가중치 0.10) : '세부/간략' 등 요구된 요약 수준에 맞는가
  
  [총점 산출 방식]  
  - 각 항목의 점수를 0~10 사이 값으로 매기고, 가중치를 곱합니다.  
  - 모든 항목의 "(점수 × 가중치)" 합을 "(가중치 총합)"으로 나눕니다.  
  - 이 값을 다시 10으로 나누어 0~1 범위로 정규화합니다.  
  - 최종 총점은 **0~1 범위 소수점 2자리**로 표시하세요.  
  - 예: 총점 = (Σ(점수 × 가중치) / Σ(가중치)) / 10  

  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}

  [평가 결과]
  평가 결과는 반드시 아래와 같은 **JSON** 양식으로 제공해줘야 합니다.
  양식은 절대 변경하지 마세요.
  ```json
  {
    "핵심 포함 여부" : <0-10 점수>,
    "불필요 정보 배제" : <0-10 점수>,
    "간결성" : <0-10 점수>,
    "논리적 흐름" : <0-10 점수>,
    "요청 수준 적합성" : <0-10 점수>,
    "총점" : <0-1범위 소수점 2자리, 가중 평균>,
    "평가의견" : <brief reason summary>
  }
  ```

Compare_template: |
  [Role]
  당신은 대규모 언어 모델이 생성한 '비교·분석' 답변의 품질을 평가하는 전문가입니다.

  [reference_data]가 제공된다면 이를 **최우선 기준**으로 삼아 [응답 프롬프트]의 품질을 평가하세요.  
  reference_data가 없을 경우, 일반적으로 알려진 사실/지식에 기반해 평가하세요.

  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위에서 점수화하세요.  
  1. 비교 기준 명확성 (가중치 0.25) : 무엇을 기준으로 비교했는지 명확한가
  2. 균형성 (가중치 0.25) : 각 비교 대상이 공정하고 균형 있게 다루어졌는가
  3. 장단점 구분 (가중치 0.20) : 장점과 단점이 명확히 구분되어 있는가
  4. 분석 깊이 (가중치 0.15) : 단순 나열이 아니라 분석이 충분히 이루어졌는가
  5. 결론/추천 설득력 (가중치 0.15) : 최종 결론이나 추천이 합리적이고 설득력 있는가

  [총점 산출 방식]  
  - 각 항목의 점수를 0~10 사이 값으로 매기고, 가중치를 곱합니다.  
  - 모든 항목의 "(점수 × 가중치)" 합을 "(가중치 총합)"으로 나눕니다.  
  - 이 값을 다시 10으로 나누어 0~1 범위로 정규화합니다.  
  - 최종 총점은 **0~1 범위 소수점 2자리**로 표시하세요.  
  - 예: 총점 = (Σ(점수 × 가중치) / Σ(가중치)) / 10  
  
  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}
  
  [평가 결과]
  평가 결과는 반드시 아래와 같은 **JSON** 양식으로 제공해줘야 합니다.
  양식은 절대 변경하지 마세요.
  ```json
  {
    "비교 기준 명확성" : <0-10 점수>,
    "균형성" : <0-10 점수>,
    "장단점 구분" : <0-10 점수>,
    "분석 깊이" : <0-10 점수>,
    "결론·추천 설득력" : <0-10 점수>,
    "총점" : <0-1범위 소수점 2자리, 가중 평균>,
    "평가의견" : <brief reason summary>
  }
  ```

Translate_template: |
  [Role]
  당신은 대규모 언어 모델이 생성한 '언어 번역' 답변의 품질을 평가하는 전문가입니다.

  [reference_data]가 제공된다면 이를 **최우선 기준**으로 삼아 [응답 프롬프트]의 번역 품질을 평가하세요.  
  reference_data가 없을 경우, 일반적으로 알려진 문법, 의미, 자연스러운 표현에 기반해 평가하세요.

  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위에서 점수화하세요.  
  1. 정확성 (가중치 0.35) : 원문 의미가 올바르게 전달되었는가  
  2. 자연스러움 (가중치 0.25) : 번역문이 해당 언어에서 자연스럽고 읽기 편한가  
  3. 문법/표현 (가중치 0.20) : 문법 오류가 없고 적절한 어휘 표현이 사용되었는가  
  4. 스타일·톤 유지 (가중치 0.20) : 원문이 가진 톤, 분위기, 형식이 잘 반영되었는가  

  [총점 산출 방식]  
  - 각 항목의 점수를 0~10 사이 값으로 매기고, 가중치를 곱합니다.  
  - 모든 항목의 "(점수 × 가중치)" 합을 "(가중치 총합)"으로 나눕니다.  
  - 이 값을 다시 10으로 나누어 0~1 범위로 정규화합니다.  
  - 최종 총점은 **0~1 범위 소수점 2자리**로 표시하세요.  
  - 예: 총점 = (Σ(점수 × 가중치) / Σ(가중치)) / 10  

  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}
  
  [평가 결과]
  평가 결과는 반드시 아래 JSON 형식을 따르세요.  
  양식은 절대 변경하지 마세요.
  ```json
  {
    "정확성": <0-10 점수>,
    "자연스러움": <0-10 점수>,
    "문법/표현": <0-10 점수>,
    "스타일·톤 유지": <0-10 점수>,
    "총점": <0-1 범위 소수점 2자리, 가중 평균>,
    "평가의견": <brief reason summary>"
  }


Summary_template_old: |
  [Role]
  당신은 대규모 언어 모델이 작성한 '요약/요약형 콘텐츠'의 품질을 평가하는 전문가입니다.

  아래 평가기준으로 [응답 프롬프트]를 평가해주세요.[reference_data]가 있다면 [reference_data] 기반으로 평가해주세요.
  [평가기준]
  다음 기준에 따라 각 항목을 0~10 범위의 점수로 평가하세요.
  1. 핵심 포함 여부: 원문에서 중요한 정보가 모두 포함되었는가
  2. 불필요 정보 배제: 사소하거나 관련 없는 내용이 제거되었는가
  3. 간결성: 불필요한 장황함 없이 간결하게 작성되었는가
  4. 논리적 흐름: 요약된 내용이 자연스럽게 이어지는가
  5. 요청 수준 적합성: '세부/간략' 등 요구된 요약 수준에 맞는가
  
  [추가 평가 기준]
  {% for c in criteria_list %}
  {{loop.index}}. {{c.name}}: {{c.description}} (최대 점수: {{c.max_score}}) (가중치: {{c.weight}}) 
  {% endfor %}

  [reference_data]
  {{reference_data}}

  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}
 

  [평가 결과]
  평가 결과는 아래와 같은 JSON 양식으로 제공해줘야 합니다.
  양식은 변경하지 마세요.
  ```json
  {
    "핵심 포함 여부" : <0-10 점수>,
    "불필요 정보 배제" : <0-10 점수>,
    "간결성" : <0-10 점수>,
    "논리적 흐름" : <0-10 점수>,
    "요청 수준 적합성" : <0-10 점수>,
    "총점" : <0-1범위 소수점 2자리, 가중 평균>,
    "평가의견" : <brief reason summary>
  }
  ```


create_criteria_prompt: |
  role: 당신은 대규모 언어 모델(LLM)이 생성한 답변의 품질을 평가하는 전문가입니다. 요청프롬프트와 LLM Response를 참고해 답변품질을 측정하기 위한 
  평가지표 5개와 조건을 만들어주세요.

  [reference_data]
  {{reference_data}}

  [Request Prompt]
  루빅스 큐브를 풀기 위한 유용한 전략은 무엇입니까?

  [Response Prompt]
  루빅스 큐브를 풀기 위한 유용한 전략은 무엇입니까?

  아래양식으로 답을주세요.
  아래양식을 변경하지 마세요.
  {평가기준 : 설명, 점수, 가중치}
          
Dinamic_criteria_prompt: |
  [role]
  당신은 대규모 언어 모델(LLM)이 생성한 답변의 품질을 평가하는 전문가입니다. 
  제공된 원래 프롬프트에 대한 모델응답을 바탕으로 LLM의 답변 품질을 정확하게 평가해 주세요.
  평가 기준을 아래와 같이 만들어주세요.
  
  [평가 기준]
  {% for c in criteria_list %}
  {{loop.index}}. {{c.name}}: {{c.description}} (최대 점수: {{c.max_score}}) (가중치: {{c.weight}}) 
  {% endfor %}

  평가 결과는 아래와 같은 JSON 양식으로 제공해줘야 합니다.
  양식은 변경하지 마세요.
  ```json
  {
      평가기준: score,
      Comment : <brief reason summary>
  }
  ```
  제공되는 요청 프롬프트와 응답 프롬프트는 아래와 같습니다.
  
  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}
  
meeting_agent_criteria_prompt: |
  [role]
  """당신은 회의 스크립트를 기반으로 작성된 회의록의 품질을 평가하는 고급 언어 모델입니다. 
  회의 스크립트와 회의록 결과를 비교하여 평가기준 별 품질을 평가하는 것이 당신의 과제입니다. 
  10점이 가장 높은 점수이며, 1점에서 10점까지의 점수를 부여하십시오. 평가가 객관적이고 건설적인 피드백을 제공하는지 확인하십시오.

  제공되는 사용자 입력 정보는 아래와 같습니다.
  [사용자입력 정보]  
   • 회의 목적: {회의 목적 입력}  
   • 요약 수준: {요약 수준 입력} (예: 기본 / 세부)  
   • 회의 스크립트 원문:  
     {회의 스크립트}
   • 생성된 회의록:  
     {모델이 생성한 회의록}

  [공통평가]
   1. 정확성 (Accuracy)	
   - 회의 발언 내용을 사실대로 잘 요약했는가? 잘못된 정보나 왜곡은 없는가?
   2. 완전성 (Completeness)	
   - 핵심 논의 내용과 결정사항, 주요 발언자 내용이 빠짐없이 포함되어 있는가?
   3. 간결성 (Conciseness)	
   - 불필요하게 장황하지 않고 요점 위주로 잘 정리되었는가?
   4. 구조화 (Structure)	
   - 논리적인 순서로 정리되었는가? (예: 개요 → 논의내용 → 결정사항 → 할당사항)
   5. 가독성 (Readability)	
   - 문장 구성이 명확하고 쉽게 읽히는가? 오탈자나 문법 오류는 없는가?

  [평가 기준]
  {% for c in criteria_list %}
  {{loop.index}}. {{c.name}}: {{c.description}} (최대 점수: {{c.max_score}}) (가중치: {{c.weight}}) 
  {% endfor %}

  [평가기준]  
   10점이 가장 높은 점수이며, 1점에서 10점까지의 점수를 부여하십시오. 각 항목에 대해 간단한 코멘트를 작성하세요.  
   마지막에 5개 항목 점수의 **평균 총점**을 계산해 주세요.
   1. 정보 충실도 (Coverage)  
   - 회의의 핵심 내용과 논의 사항, 발언, 결정사항이 누락 없이 반영되었는가?
   2. 요약 적절성 (Summarization Quality)  
   - 요약 수준(기본/세부)에 맞게 적절한 정보만 간결하게 추려졌는가?
   3. 형식 일관성 (Format Consistency)  
   - 회의록 양식(제목, 참석자, 논의 내용, Action Items 등)을 잘 따랐는가?
   4. 의미 왜곡 여부 (Semantic Fidelity)  
   - 스크립트의 발언이나 결정을 잘못 해석하거나 왜곡하지 않았는가?
   5. 사용자 입력 반영도 (User Input Alignment)  
   - ‘회의 목적’, ‘요약 수준’ 등 사용자 요구사항을 충실히 반영했는가?
  
  **평가 기준:**
  1. **참조 답변과의 비교:** AI가 추출한 정보가 참조 답변과 얼마나 정확하게 일치합니까?
  2. **관련성:** 답변이 문서를 기반으로 사용자의 질문을 효과적으로 다루고 있습니까?
  3. **유창성:** 답변이 자연스럽고 유창하게 작성되었습니까?
  4. **자연스러움:** 답변이 자연스럽고 자연스러운 표현을 사용하고 있습니까?
  
  평가 결과는 아래와 같은 JSON 양식으로 제공해줘야 합니다.
  양식은 변경하지 마세요.
  ```json
  {
      평가기준: <score from 0 to 10>,
      Comment : <brief reason summary>
  }
  ```
 
  [요청 프롬프트]
  {{prompt}}
  
  [응답 프롬프트]
  {{response}}
  

multi_criteria_prompt: |
  다음 응답을 아래 평가 기준별로 모두 평가하고 가중치를 적용한 총점을 0-1 사이에 소수점 3자리까지 점수를 넣어주세요.
  
  [원래 프롬프트]
  {{prompt}}
  
  [모델 응답]
  {{response}}
  {% if reference_answer %}
  [참조 답변]
  {{reference_answer}}
  {% endif %}
  [평가 기준]
  {% for c in criteria_list %}
  {{loop.index}}. {{c.name}}: {{c.description}} (최대 점수: {{c.max_score}}) (가중치: {{c.weight}}) 
  {% endfor %}

  각 기준별로 아래의 JSON형식으로 제공해 주세요.
  {
      "기준": 평가 기준,
      "점수": 점수,
      "피드백": 피드백,   
      "총점": 총점 
  }


single_criteria_prompt: |
  [role]
  당신은 대규모 언어 모델(LLM)이 생성한 답변의 품질을 평가하는 전문가입니다. 
  제공된 원래 프롬프트에 대한 모델응답을 바탕으로 LLM의 답변 품질을 정확하게 평가해 주세요.
 
  [원래 프롬프트]
  {{prompt}}
  
  [모델응답]
  {{response}}
  
  [instruction]
   - 프롬프트와 모델응답을 면밀히 검토해, 아래 5개의 항복별로 각각 해당하는 점수를 소수점 2자리(ex. 0.83) 수치로 부여해 주세요. 
   - 아래 평가항목에 대해 Accuracy, Consistency, Understanding, Completeness, Requirements 평가
   - OverallScore = Accuracy + Consistency + Understanding + Completeness + Requirements
   - OverallScore를 부여한 이유에 대해 OverallReason에 설명해 주세요. 
   - OverallScore 계산식은 변경하지 말고 정확히 해주세요.


      1. Accuracy: LLM 답변에서 제공하는 정보가 정확한지를 확인합니다. 틀린 정보를 제공하거나 모른다는 답변은 피해야 합니다.(가중치:1.5)
          - 틀린 정보를 포함하지는 않는가?
          - 정확한 정보를 바탕으로 답변이 생성되었는가?
          - 관련 없는 정보는 활용되지 않았는가?
          - 수치, 지역명, 상품명, 공고명 등 구체적 정보가 정확한가?
          
      2. Consistency: LLM 답변이 일관성 있는 정보를 제공하는지 확인해야 합니다. 정보가 상호모순되거나 약간이라도 잘못된 정보를 포함하면 안됩니다.(가중치:1.0)
          - 내부 문장 간 논리적 모순이나 상충 내용 없는가?
          - 문맥에 어긋나는 문장이나 문단은 없는가?

      3. Understanding: 질문의 문맥을 이해하고 그에 맞는 LLM 답변을 제공하는 능력이 중요합니다. 문맥을 고려하지 않고 일반적인 답변만을 제공하면 안됩니다.(가중치:1.3)
          - 질문의 의도를 정확하게 파악했는가?
          - 질문과 관련된 대답을 하고 있는가?
          - 질문에 대한 정보를 모두 포함하고 있고, 틀리지 않았는가?
      
      4. Completeness: LLM 답변이 주어진 질문에 충분한 정보를 포함하는지 여부가 중요합니다. 질문이 여러개 이면, 여러 질문에 대해 모두 답변해야 합니다.(가중치:1.0)
          - 질문의 내용을 모두 포함하고 있는가?
          - 불필요한 부연 설명 없이 직접적인 답변 제공하는가?    

      5. Requirements: 다음의 요구사항은 반드시 만족시켜야 하는 조건입니다. (가중치:0.5)
          - 질문에 대한 답변이 추상적이지 않고, 명확하고 구체적인가?
          - 주어진 정보로 알 수 없다고 대답하거나, 답변을 회피하지 않는가?

    [제약 조건] 
    피드백 코멘트는 사용자 질문과 동일한 언어로 작성해야 합니다.
    설명은 간결하고 객관적이어야 합니다.
    JSON 출력 외에 설명이나 추론을 포함하지 마세요.

    [출력 형식]
    오직 JSON 객체만 반환합니다(추가 키나 설명을 추가하지 마세요).

    ```json
    {
    Accuracy: score,
    Consistency: score,
    Understanding: score,
    Completeness: score,
    Requirements: score,
    OverallScore : score,
    Comment : <brief reason summary>
    }


    입력프롬트트와 응답은 아래와 같습니다.
    마크다운 형태로 변경
    이름정하기
    judge template, criteria
    평가기준 프롬프트 자동화
    회의록 에이전트에 시스템 프롬프트
    사람이 평가한 데이터가 있다. 
    프롬프트별 평가기준을 gpt한테 물어보고 평가기준을 만들어줘야 함. 
    




other_criteria_prompt: |
    # Role
    You are an impartial expert AI judge for evaluating assistant chat completions.

    # Task
    Evaluate the quality of the assistant's last response based on the provided conversation history.

    # Inputs
    - **messages**: The full conversation history (list of role/content pairs).
    - **assistant_response**: The model's answer to the last user message.

    # Evaluation Criteria
    Evaluate according to the following four criteria:

    ## 1. Accuracy (0‑3)
    - 0: completely incorrect
    - 1: partially correct but with errors
    - 2: mostly correct
    - 3: entirely correct

    ## 2. Relevance (0‑2)
    - 0: irrelevant
    - 1: partially relevant
    - 2: fully relevant

    ## 3. Clarity (0‑2)
    - 0: unintelligible
    - 1: understandable but awkward
    - 2: clear and natural

    ## 4. Completeness (0‑3)
    - 0: no content addressing request
    - 1: addresses only part
    - 2: mostly covers
    - 3: fully satisfies all requirements

    # Constraints
    The comments field must be written in the same language as the user's question.
    Be concise and objective in your explanation.
    Do not include explanations or reasoning outside of the JSON output.

    # Output Format
    Return **only** the following JSON object (do not add any extra keys or explanations):

    ```json
    {
    "accuracy": <0‑3>,
    "relevance": <0‑2>,
    "clarity": <0‑2>,
    "completeness": <0‑3>,
    "comments": "<brief reason summary>"
    }


otherh_criteria_prompt: |
    # 역할
    당신은 어시스턴트 채팅 완료를 평가하는 공정한 전문가 AI 심사위원입니다.

    # 작업
    제공된 대화 기록을 기반으로 어시스턴트의 마지막 응답 품질을 평가합니다.

    # 입력
    - **messages**: 전체 대화 기록(역할/콘텐츠 쌍 목록).
    - **assistant_response**: 마지막 사용자 메시지에 대한 모델의 답변.

    # 평가 기준
    다음 네 가지 기준에 따라 평가하십시오.

    ## 1. 정확성 (0~3)
    - 0: 완전히 틀림
    - 1: 부분적으로 맞지만 오류가 있음
    - 2: 대부분 맞음
    - 3: 완전히 맞음

    ## 2. 관련성 (0~2)
    - 0: 무관
    - 1: 부분적으로 맞음
    - 2: 완전히 맞음

    ## 3. 명확성 (0~2)
    - 0: 이해할 수 없음
    - 1: 이해할 수 있지만 어색함
    - 2: 명확하고 자연스러움

    ## 4. 완전성 (0~3)
    - 0: 콘텐츠 주소 요청 없음
    - 1: 일부만 다룸
    - 2: 대부분 다룸
    - 3: 모든 요구 사항을 완전히 충족

    # 제약 조건
    댓글 필드는 사용자 질문과 동일한 언어로 작성해야 합니다.
    설명은 간결하고 객관적이어야 합니다.
    JSON 출력 외에 설명이나 추론을 포함하지 마세요.

    # 출력 형식
    다음 JSON 객체 **만** 반환합니다(추가 키나 설명을 추가하지 마세요).

    ```json
    {
    "accuracy": <0‑3>,
    "relevance": <0‑2>,
    "clarity": <0‑2>,
    "completeness": <0‑3>,
    "comments": "<간략한 이유 요약>"
    }
